---
layout: post
title:  "DeepSeek's AI Efficiency Revolution - Implications for Clean Energy"
date:   2025-02-10T00:00:00-00:00
author: Cece
categories: Clean-Energy-Insights
tags: Industry
---

DeepSeek, a Chinese AI company, has taken the AI industry by storm recently, releasing two ground-breaking models in a row.

DeepSeek-V3 was released quietly on December 26, 2024, right after OpenAI's theatrical 12-day Christmas gift season. Deviating from the Dense Model used by OpenAI and other leading Large Language Model (LLM) providers such as Enthropic, [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) uses an innovative "Mixture-of-Experts (MoE)" architecture, enabling model training at stunning efficiency. By activating only a subset of the parameters during inference, this design allows upsizing the model, hence the output quality and inference speed, without raising the costs. Impressively, the V3 model was developed for under $6 million in just two months under the constraints of using the older-generation, less powerful GPU chips due to US export restrictions. 

It wasn't until the AI community had fully caught up with the implication of DeepSeek-V3 that DeepSeek released another model, [DeepSeek-R1](https://api-docs.deepseek.com/news/news250120), at the beginning of 2025. Trained on top of V3 with a Reinforcement-Learning-based, Chain-of-Thought layer, this reasoning model set another record with superior reasoning ability through high-speed, low-latency processing at an even smaller model size.

Despite the smaller models, lower development costs, and infrastructure constraints, DeepSeek's models achieved nearly on-par, if not better, performance compared to the leading LLM players. The latest R1 model competes directly with OpenAI's o1 in benchmarks and outperforms several major models, including Google's Gemini 2.0 Flash and Claude 3.5 Sonnet.

### **The Dance Between AI and Clean Energy**

With DeepSeek bringing both awe and fear to the AI community, it also shook the clean energy industry. A large chunk of the clean energy deployed in the past two years was driven by the latest AI boom, particularly by hyperscalers racing to train the largest and greatest models while trying to meet the new power demand with clean energy supply, thanks to their commitments to their net-zero pledges. Among the major hyperscalers, [Amazon](https://sustainability.aboutamazon.com/climate-solutions/carbon-free-energy) leads the pack, having achieved 100% renewable energy in 2023 and aiming to become carbon-neutral by 2040. The other three hyperscalers have 2030 in mind, including [Microsoft](https://blogs.microsoft.com/blog/2020/01/16/microsoft-will-be-carbon-negative-by-2030/)'s plan to become carbon-negative by 2030, [Google](https://sustainability.google/operating-sustainably/net-zero-carbon/)'s goal to achieve 24/7 carbon-free operations by 2030, and [Meta](https://sustainability.atmeta.com/climate/)'s commitment to get to net zero by 2030. As a result, record investments were poured into procuring and deploying clean energy, with solutions ranging from intermittent renewables, such as solar and wind, to energy storage solutions, such as batteries and hydrogen, to baseload resources, such as hydro and nuclear. A powerful feedback loop was formed between advancing AI and scaling clean energy innovations.

Much of the investment enthusiasm going into clean energy, however, was built on the premise that the energy demand would grow exponentially following the scaling trajectory of LLMs developed by the industry leaders thus far. In other words, the size of the language model holds the key to the AI revolution: the larger the model gets, the better the performance will be - all the way till AGI, as OpenAI would like to make you believe. DeepSeek challenged this assumption. By achieving state-of-the-art performance training smaller models with just $6 million and less powerful hardware, DeepSeek suggests that AI development might not require the massive resources previously thought necessary - a smaller model, coupled with reinforcement learning and other techniques, can be just as mighty. This led to cooling investments in not just the AI darlings from the previous cycle but also the clean energy sector as investors questioned previous projections of exponential power demand growth fueled by AI.

### **The Sudden Fame of Jevons**

Will the disruption of DeepSeek lead to an alternative AI future that demands less power and clean energy? Likely no. Enter Jevons paradox.

Jevons paradox describes the phenomenon where improved resource efficiency often leads to increased total consumption through broader adoption. When Watt's steam engine improved efficiency threefold in the 1800s, coal consumption increased tenfold as new applications emerged. Similarly, LED lighting's 85% efficiency gain since 2000 led to more total energy use through expanded applications. Computing itself shows this pattern: despite exponential efficiency improvements under Moore's Law, total energy consumption grew as computers became ubiquitous.

At its core, Jevons paradox underlines the distinct roles played by infrastructure and applications and how they interlink. A revolutionary technology almost always starts at the infrastructure layer and moves to the application layer only after the basic infrastructure is in place. In practice, the threshold is reached when building a useful application becomes accessible to an average 'smart' person who may not be at the cutting edge of the technology and knows the field from the inside out. Then, the scaling begins.

This relationship applies in this case because we're still early on in the AI revolution: despite the hype of AGI and AI taking over the world, we are not there. Most of the development centered around infrastructure. A new breakthrough in LLM happens almost every week. While exciting, it indicates that the infrastructure layer is far from stabilization. Most startups who attempted to build AI applications using an LLM wrapper have failed miserably in the last two years, further proving the fragility of the current AI infrastructure. Those that survived either combine traditional ML with LLM, use ML only, or sprinkle some AI on top of the old-school technology stack without actually using AI.

By no means is DeepSeek necessarily the answer to the AI infrastructure. DeepSeek, however, demonstrates that AI computing isn't just about throwing money at the problem and building ever-larger models. It shows that AI infrastructure development doesn't have to be monopolized by tech giants or generously funded research labs and that smaller companies also have a chance to shape the landscape. It creates a pathway to build cheaper and better AI infrastructure and moves the discussion from simply scaling AI infrastructure to building AI applications.

### **Investment Implications**

In fact, rather than a change in the fundamental thesis of AI and clean energy, most of the recent market reactions reflect a healthy correction from last year's AI hype. DeepSeek hasn't broken the promise of a clean energy AI future - it created an alternative path. The following era will likely see two trends playing out and reinforcing each other, including continued optimization of AI infrastructure, hence energy-efficient AI model training, and, in reverse, the expansion of AI applications built on top of the enhanced infrastructure to drive 24/7 carbon-free energy deployment. 

**Specifically, there will be a continued push to improve energy efficiency at the infrastructure level.**

The trend of improving AI data processing and transmission efficiency to reduce power demand will continue. 

- Much of the R&D in the past few years has been focusing on *data processing*. In addition to Nvidia's dominating technology, startups such as Croq continue to push the envelope through specialized AI chips tailored to specific AI models. Advanced features such as dynamic voltage scaling and selective activation architectures are designed to optimize power usage and achieve faster processing.
- Increasingly, companies are shifting gears to *data transmission*. Promising technologies, such as silicon photonics, emerge to tackle both fronts by enabling optical computing while simultaneously serving as a low-power data transmission medium, attracting major capital to back the innovation. Integrating photonics through co-packaged optics and bringing high-bandwidth communication directly to the chip level is expected to not just improve the processing speed but also reduce the power consumed for moving data. Through wavelength division multiplexing, the technology can also enable efficient rack-to-rack communication, resulting in significant power reductions in AI data centers.

On the other hand, while efficient data processing and transmission can reduce power consumption at the computational core, this may not lead to overall energy reduction as more energy may be needed for cooling. Currently, up to [40%](https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/investing-in-the-rising-data-center-economy) of the AI data center energy goes to cooling, and the percentage will likely grow as AI computing improves further. Solving the cooling challenge has to go hand in hand with AI advancement by combining solutions from multiple fronts:

- Adopting novel materials, including advanced thermal interfaces, phase change materials, and nanostructured surfaces, will enable more efficient heat transfer.
- Developing advanced cooling technologies, including direct-to-chip and liquid immersion cooling, and leveraging hybrid cooling approaches such as warm water cooling, rear-door heat exchangers, and dynamic cooling modes for varying heat loads shall increase cooling efficiency and reduce energy consumption.
- Leveraging waste heat utilization solutions, including integration with local district heating systems and providing heat for nearby buildings and industrial processes, can further reduce energy waste.

**As infrastructure advancement accelerates, the stable foundation will usher in a new wave of AI applications, the adoption of which shall further accelerate clean energy development.**

- On the power generation side, AI can revolutionize power generation through advanced forecasting and optimization. Machine learning models improve renewable energy forecasting accuracy by 30-40%, enabling better grid integration and reducing the need for backup power. During operations, AI models can optimize equipment performance, predict maintenance needs, and adjust operations based on weather conditions. In battery technology, AI algorithms optimize charging protocols, predict maintenance needs, and extend battery life by analyzing degradation patterns and adjusting operations in real time.
- The transmission and distribution system can benefit in a similar way. Advanced algorithms analyze vast sensor networks to predict and prevent equipment failures, optimize power flow, and enable real-time grid balancing. This is especially important as more renewable energy sources come online, challenging grid stability with intermittency. Many of the new bottlenecks, including land constraints and interconnection queueing optimization, can also be eased by utilizing data and models to manage the process.
- AI plays a big role in demand management. Startups such as WeaveGrid have already started commercializing AI inference tools to predict and manage demand patterns across industrial, commercial, and residential sectors for load shifting and demand optimization. For the large load coming from C&I, including data centers, leveraging AI to optimize process scheduling and equipment operation can greatly reduce energy consumption. The era of "prosumer" presents even more AI use cases, including AI-coordinated distributed resources to balance supply and demand across thousands of points in real time, and AI-orchestrated two-way charging systems as more EVs are expected to be connected to the grid for demand management.

### **Final Thought**

DeepSeek's breakthrough, far from diminishing the need for clean energy, marks a turning point in how AI will consume power. While their efficient architecture challenges the "bigger is better" model paradigm, it paradoxically points toward increased overall energy demand through widespread AI adoption. Instead of powering a handful of massive models in centralized data centers, future energy needs will support thousands of specialized AI applications distributed across the economy – from grid management to industrial processes, from buildings to transportation systems.

For the clean energy sector, this isn't a threat – it's a mandate for innovation. The future demands not just more clean power, but smarter, more flexible energy systems capable of supporting AI's evolution from a luxury of tech giants to the backbone of our economy. DeepSeek hasn't dampened the clean energy transition; they've handed it a new blueprint for growth.