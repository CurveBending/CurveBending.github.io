---
layout: post
title:  "DeepSeek's AI Efficiency Revolution - Implication for Clean Energy"
date:   2025-02-10T00:00:00-00:00
author: Cece
categories: Clean-Energy-Insights
tags: Industry
---

DeepSeek, a Chinese AI company, has recently disrupted the AI industry by developing highly efficient large language models that compete with top US competitors at a fraction of the cost. The company's innovative ["mixture-of-experts" architecture](https://github.com/deepseek-ai/DeepSeek-V3) divides its 671 billion parameter model into specialized submodels, using only 37 billion parameters at a time while employing inference-time compute scaling to adjust computational effort based on task complexity. This efficient design, combined with their novel "mixed precision" framework that strategically uses both FP32 and FP8 calculations, allowed them to build their V3 model for under $6 million in just two months, even while using less powerful H800 chips due to US export restrictions.

For a fraction of the cost compared to their US peers, DeepSeek's models, however, achieved nearly on-par performance. Their R1 model competes directly with OpenAI's o1 in benchmarks and outperforms several major models, including Google's Gemini 2.0 Flash and Claude 3.5 Sonnet. Their latest multimodal model, Janus-Pro-7B, claims to surpass DALL-E and Stable Diffusion 3 in multiple benchmarks. This success suggests that AI development might not require the massive resources previously thought necessary, potentially marking a shift in how the industry approaches model development and efficiency.

### **The Dance Between AI and Clean Energy**

With DeepSeek bringing both awe and fear to the AI community, it also shook the clean energy industry. A large chunk of the clean energy deployed in the past two years was driven by the latest AI boom. In particular, as hyperscalers race to train the largest and greatest models, the unprecedented new power demands need to be met by clean energy, given the net-zero pledges the majority of the hyperscalers have committed to. Having achieved 100% renewable energy in 2023, [Amazon](https://sustainability.aboutamazon.com/climate-solutions/carbon-free-energy) aims to become carbon-neutral by 2040. The other three hyperscalers have 2030 in mind, including [Microsoft](https://blogs.microsoft.com/blog/2020/01/16/microsoft-will-be-carbon-negative-by-2030/)'s plan to become carbon-negative by 2030, [Google](https://sustainability.google/operating-sustainably/net-zero-carbon/)'s goal to achieve 24/7 carbon-free operations by 2030, and [Meta'](https://sustainability.atmeta.com/climate/)s commitment to get to net zero by 2030. Record investments were poured into the space, deploying solutions ranging from intermittent renewable, including solar and wind, to energy storage solutions, including batteries and hydrogen, to baseload resources, including hydro and nuclear. A powerful feedback loop was formed between advancing AI and scaling clean energy innovations.

Much of the investment enthusiasm going into clean energy, however, was built on the premise that the energy demand would grow exponentially following the scaling trajectory of the large language model. In other words, the size of the language model holds the key to the AI revolution: the larger the model gets, the better the performance will be - all the way till AGI, as OpenAI would like to make you believe. DeepSeek challenged this assumption. By achieving state-of-the-art performance with just $6 million in training costs and less powerful hardware, DeepSeek demonstrated that a smaller model, coupled with reinforcement learning and other traditional techniques, can be just as mighty. The new finding sent the market value of leading AI companies such as Nvidia to a freefall and led to cooling investments in clean energy markets as investors questioned previous projections of exponential growth in AI power demand.

### **The Sudden Fame of Jevons**

Technologists like to use smart-sounding terms to explain things. This time, Jevons paradox took center stage.

Jevons paradox describes the phenomenon where improved resource efficiency often leads to increased total consumption through broader adoption. When Watt's steam engine improved efficiency threefold in the 1800s, coal consumption increased tenfold as new applications emerged. Similarly, LED lighting's 85% efficiency gain since 2000 led to more total energy use through expanded applications. Computing itself shows this pattern: despite exponential efficiency improvements under Moore's Law, total energy consumption grew as computers became ubiquitous.

Underneath Jevons paradox is the distinct roles played by infrastructure and applications. A revolutionary technology almost always starts at the infrastructure layer and moves to the application layer only after the basic infrastructure is in place. In practice, the threshold is reached when building a useful application becomes accessible to an average 'smart' person who may not be at the cutting edge of the technology and knows the field from the inside out. Then, the scaling begins.

Jevons paradox applies in this case because we're still early on in the AI revolution: despite the hype of AGI and AI taking over the world, we are not there. Most of the development centered around infrastructure. A new breakthrough in LLM happens almost every week. While exciting, it indicates that the infrastructure layer is far from stabilization. Most startups who attempted to build AI applications using an LLM wrapper have failed miserably in the last two years, further proving the fragility of the current AI infrastructure. Those that survived either combine traditional ML with LLM, use ML only, or sprinkle some AI on top of the old-school technology stack without actually using AI.

By no means is DeepSeek necessarily the answer to the AI infrastructure. In fact, being a reasoning model itself, while DeepSeek may lead in reasoning-related performance, it lags in many other areas. DeepSeek, however, demonstrates that AI computing isn't just about throwing money at the problem and building ever-larger models. It shows that AI infrastructure development doesn't have to be monopolized by tech giants or generously funded research labs, and that smaller companies also have a chance to shape the landscape. By creating a pathway to build cheaper and better AI infrastructure, it moves the discussion from simply scaling AI infrastructure to building AI applications.

### **Investment Implications**

In fact, rather than a change in the fundamental thesis of AI and clean energy, most of the recent market reactions reflect a healthy correction from last year's AI hype. DeepSeek hasn't broken the promise of a clean energy AI future - it created an alternative path. The following era will likely see two trends playing out and reinforcing each other, including continued optimization of AI infrastructure, hence energy-efficient AI model training, and, in reverse, the expansion of AI applications built on top of the enhanced infrastructure to drive 24/7 carbon-free energy deployment. 

**Specifically, there will be a continued push to improve energy efficiency at the infrastructure level.**

The trend of improving AI data processing and transmission efficiency to reduce power demand will continue. 

- Much of the R&D in the past few years has been focusing on data processing. In addition to Nvidia's dominating technology, startups such as Croq continue to push the envelope through specialized AI chips tailored to specific AI models. Advanced features such as dynamic voltage scaling and selective activation architectures are designed to optimize power usage and achieve faster processing.
- Increasingly, companies are shifting gears to data transmission. Promising technologies, such as silicon photonics, emerge to tackle both fronts by enabling optical computing while simultaneously serving as a low-power data transmission medium, attracting major capital to back the innovation. Integrating photonics through co-packaged optics by bringing high-bandwidth, low-latency communication directly to the chip level is expected to not just improve the processing speed but also reduce the power consumed for moving data. Through wavelength division multiplexing, the technology can also enable efficient rack-to-rack communication, resulting in significant power reductions in AI data centers.

While efficient data processing and transmission can reduce power consumption at the computational core, this may not lead to overall energy reduction as more energy may be needed for cooling. Currently, up to [40%](https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/investing-in-the-rising-data-center-economy) of the AI data center energy goes to cooling, and the number will likely grow as AI computing improves further. Tackle this problem requires designing solutions at the system level: 

- Adopting novel materials, including advanced thermal interfaces, phase change materials, and nanostructured surfaces, will enable more efficient heat transfer.
- Developing advanced cooling technologies, including direct-to-chip and liquid immersion cooling, and leveraging hybrid cooling approaches such as warm water cooling, rear-door heat exchangers, and dynamic cooling modes for varying heat loads shall increase cooling efficiency and reduce energy consumption.
- Leveraging waste heat utilization solutions, including integration with local district heating systems and providing heat for nearby buildings and industrial processes, can further reduce energy waste.

**The infrastructure enhancement will bring a new wave of AI** 

**applications, whose adoption shall further accelerate clean energy development.**

- On the power generation side, AI can revolutionize power generation through advanced forecasting and optimization. Machine learning models improve renewable energy forecasting accuracy by 30-40%, enabling better grid integration and reducing the need for backup power. During operations, AI models can optimize equipment performance, predict maintenance needs, and adjust operations based on weather conditions. In battery technology, AI algorithms optimize charging protocols, predict maintenance needs, and extend battery life by analyzing degradation patterns and adjusting operations in real time.
- The transmission and distribution system can benefit in a similar way. Advanced algorithms analyze vast sensor networks to predict and prevent equipment failures, optimize power flow, and enable real-time grid balancing. This is especially important as more renewable energy sources come online, challenging grid stability with intermittency. Many of the new bottlenecks, including land constraints and interconnection queueing optimization, can also be eased by utilizing data and models to manage the process.
- AI plays a big role in demand management. Startups such as WeaveGrid have already started commercializing AI inference tools to predict and manage demand patterns across industrial, commercial, and residential sectors for load shifting and demand optimization. For the large load coming from C&I, including data centers, leveraging AI to optimize process scheduling and equipment operation can greatly reduce energy consumption. The era of 'pro-sumer' presents even more AI use cases, including AI coordinated distributed to balance supply and demand across thousands of points in real time, and AI orchestrated two-way charging system as more EVs are expected to be connected to the grid for demand management.

### **Final Thought**

The DeepSeek breakthrough, far from slowing down clean energy momentum, has simply shifted the focus of AI's energy demands. While the era of exponentially scaling model sizes may be reaching its natural limits, the overall energy requirements for AI will likely continue to grow as we move from the infrastructure phase to widespread application deployment. As AI becomes embedded in everything from grid management to industrial processes, from building controls to transportation systems, the demand for clean, reliable power will only intensify. The key difference is that this energy will be powering thousands of specialized, efficient AI applications rather than a handful of massive models. This evolution suggests that the clean energy sector's growth trajectory remains strong—it's not a question of if we'll need more clean power, but rather how we'll use it most effectively to enable AI's next phase of development across every sector of the economy.